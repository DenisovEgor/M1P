\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Адаптация методов обучения с подкреплением в задаче обучения LLM}

\author{\textbf{Денисов Егор Александрович} \\
	МГУ им. М.В.Ломоносова \\
	Факультет ВМК, кафедра ММП\\
	Москва, Россия \\
	\texttt{s02220081@gse.cs.msu.ru} \\
	%% examples of more authors
	\And
	\textbf{Сердюк Юлиан Анатольевич} \\
	МГУ им. М.В.Ломоносова \\
	Факультет ВМК, кафедра ММП \\
	Москва, Россия \\
	% \texttt{stariate@ee.mount-sheikh.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	Современные большие языковые модели (БЯМ) демонстрируют впечатляющие способности к рассуждению, обобщению и генерации текста, однако их эффективность во многом зависит от качества этапов дообучения — дообучения с учителем (англ. SFT) и обучения с подкреплением (англ. RL). Несмотря на то, что данные подходы обширно изучаются, остаются открытыми вопросы о том, как корректно их совмещать для достижения наилучшего качества. В данной работе рассматриваются подходы к адаптации методов обучения с подкреплением для задач дообучения БЯМ с особым вниманием к двум аспектам - комбинации функции потерь и оптимальному чередованию этапов обучения. Эксперименты выполняются на стандартных бенчмарках для тестирования математического и логического рассуждения модели. Анализ результатов показывает, что гибридные схемы, предложенные нами, обеспечивают лучшее соотношение между устойчивостью, скоростью сходимости и способностью к обобщению.
\end{abstract}


% \keywords{First keyword \and Second keyword \and More}

\section{Introduction}
Современные большие языковые модели (БЯМ) становятся основным инструментом в задачах обработки естественного языка и генерации текста. Однако для достижения высокой точности и способности к обобщению им необходимо сложное многоэтапное обучение, включающее в себя различные этапы. Актуальность исследований в этой области обусловлена необходимостью повышения устойчивости, управляемости и эффективности БЯМ при решении задач, требующих рассуждений и логических выводов.

На сегодняшний день одним из ключевых направлений развития БЯМ является совершенствование этапов дообучения, в частности SFT и RL. Основной подход на стадии RL - обучение с подкреплением на основе человеческих ответов (англ. RLHF)(\citep{RLHF, PPO, DPO, GRPODeepSeek}), а также различные его улучшения. В качестве последних выступают различные модификации функции вознаграждения, стратегий обновления модели и схем совмещения RL с традиционным SFT.

Тем не менее, существующие решения имеют ряд ограничений. Во-первых, большинство подходов не учитывают сложное взаимодействие между этапами SFT и RL, что может приводить к забыванию уже приобретённых моделью знаний (\citep{Forgetting}). Во-вторых, функции вознаграждения зачастую плохо коррелируют с реальными показателями качества текста, особенно в задачах рассуждения. В-третьих, в RLHF используются данные собранные в онлайн-режиме. Зачастую такие данные ограничены, и используемые методы оптимизации нередко страдают от нестабильности и переобучения.

В данной работе предлагается подход к адаптации методов обучения с подкреплением для дообучения БЯМ, основанный на гибридной схеме чередования этапов SFT и RL с комбинированной функцией потерь. Такой подход позволяет учитывать как сигналы от данных с учителем, так и награды, поступающие из среды. Особое внимание уделяется исследованию стратегий управления частотой переходов между режимами обучения и влиянию этих переходов на стабильность оптимизации. Кроме того, рассматриваются модификации функции вознаграждения, направленные на повышение чувствительности к ошибкам логического вывода.

Модели, обученные с использованием предложенного подхода, демонстрируют более быструю сходимость и лучшую способность к обобщению по сравнению с классическим RLHF. Таким образом, представленные результаты вносят вклад в развитие методологий дообучения БЯМ, предлагая более гибкий и устойчивый механизм совмещения SFT и RL, что открывает новые перспективы для дальнейшего повышения качества языковых моделей.


\section{Related works}
\subsection{Парадигмы обучения}
Дообучение с учителем является основополагающей техникой для адаптации больших языковых моделей под специфичные задачи. Благодаря своей эффективности, данный подход стал широко применим в различных областях, в том числе и в решении математических задач (\citep{GSM8K, MATH, SFTMath}). Тем не менее, чистый SFT позволяет хорошо выучить шаблоны решений, но не помогает улучшить способность модели к рассуждению. Причиной этому служат ограниченные и несбалансированные данные, неоптимальная для данной задачи функция потерь.

Для решения существующих проблем после стадии SFT стало применяться обучение с подкреплением. Таким образом, SFT позволяет достичь высокого базового качества, а RL улучшает эффективность и подстраивает поведение модели под конкретную задачу. Ярким примером совмещения двух парадигм является \texttt{DeepSeekMath} (\citep{GRPODeepSeek})

\subsection{Совмещение SFT и RL}
Последовательное выполнение стадий SFT и RL также имеет свои ограничения и недостатки. Вследствие этого исследуются способы динамически совмещать данные подходы. Так, в работах \citep{ReLIFT, SuperRL} предлагаются схожие стратегии чередования RL и SFT, где RL служит основой обучения, а SFT применяется для наиболее сложных примеров в выборке.

Другой класс работ предлагает пошаговую адаптацию и мониторинг тренировочной динамики: SASR (\citep{SASR}) и близкие методы отслеживают градиентные нормы и распределение выходов модели, чтобы динамически корректировать вклад SFT и RL на каждом шаге обучения и тем самым уменьшить эффект катастрофического забывания и переобучения.

Наконец, в сторону более формализованных схем кооперации шагнул BRIDGE (\citep{BRIDGE}), который использует двухуровневую оптимизацию для одновременного обучения «нижнего» уровня (RL-обновления) и «верхнего» уровня (SFT). Такая кооперативная постановка направлена на то, чтобы SFT на каждой итерации предоставлял улучшенную инициализацию для стадии обучения с подкреплением, что даёт прирост итоговой точности.



\section{Постановка задачи}
\subsection{Данные}
Рассматривается датасет $\mathcal{D} = \{(x_i, y_i)\}_{i = 1}^{N}$, где $x_i$ - текстовое задание, представляющее собой условие математической задачи; $y_i$ - эталонное текстовое решение или ответ. Каждое задание и решение может рассматриваться как последовательность токенов $x_i = (x_{i.1}, \cdots, x_{i, T_{x_i}}), y_i = (y_{i.1}, \cdots, y_{i, T_{y_i}})$, где $T_{x_i}, T_{y_i}$ - длины последовательностей $x_i, y_i$ соответственно.

\subsection{Отображение}
Модель $f_{\theta}: \mathcal{X} \rightarrow \mathcal{Y}$ отображает входной текст $x$ в распределение над возможными выходными последовательностями $y$:
$$f_{\theta}(x) = arg \max_{y \in \mathcal{Y}} {\pi_{\theta}(y | x)}$$
Здесь $\pi_{\theta}(y | x) = \prod_{t = 1}^{T}{\pi_{\theta}(y_t | x, y_{<t})}$.

\subsection{Внешний критерий качества}
Для задач математического рассуждения естественным внешним критерием качества служит точность рассуждения, оцениваемая на уровне финального ответа. В данном случае таким критерием выступает метрика \textbf{Pass@k}.

\textbf{Pass@k} - это вероятность того, что среди первых $k$ решений модели есть хотя бы одно правильное. Математически, если $x$ - задача, $\{y_i\}_{i = 1}^m$ - ответы модели, $c = \sum_{i = 1}^m{\mathbb{I}[y_i \text{ - правильный ответ}]}$ - количество правильных ответов модели. Тогда \textbf{Pass@k} = $1 - \frac{C_{m - c}^k}{C_m^k}$ при условии того, что $C_{m - c}^k = 0$, если $k > m - c$

\subsection{Оптимизационная задача}
Оптимизационная задача выглядит следующим образом:
$$\theta^* = arg \min_{\theta} \mathcal{L}_{total}(\theta)$$
Здесь совокупная функция потерь определяется как
$$\mathcal{L}_{total}(\theta) = \lambda_{SFT}\mathcal{L}_{SFT}(\theta) + \lambda_{RL}\mathcal{L}_{RL}(\theta)$$
Коэффициенты $\lambda_{SFT}, \lambda_{RL} \geq 0$ регулируют вклад каждого из этапов в данный момент времени.
Компонента $\mathcal{L}_{SFT}$ соответствует стандартной кросс-энтропийной функции потерь:
$$\mathcal{L}_{SFT}(\theta) = -\mathbb{E}_{(x, y) \sim \mathcal{D}} \sum_{t = 1}^T \log \pi_{\theta}(y_t | x, y_{<t})$$
В качестве компоненты $\mathcal{L}_{RL}$ используется функция потерь метода $GRPO$ (\citep{GRPODeepSeek}): \\
$\mathcal{L}_{GRPO}(\theta) = -\mathbb{E}_{x \sim \mathcal{D}, \{y_i\}_{i = 1}^G \sim \pi_{\theta}(\cdot | x)}$
$${\frac{1}{G} \sum_{i = 1}^G {\frac{1}{T_{y_i}} \sum_{t = 1}^{T_{y_i}} \{\min [\frac{\pi_{\theta}(y_{i, t} | x, y_{i, <t})}{\pi_{\theta_{old}}(y_{i, t} | x, y_{i, <t})} \hat{A_{i, t}}, \text{clip} (\frac{\pi_{\theta}(y_{i, t} | x, y_{i, <t})}{\pi_{\theta_{old}}(y_{i, t} | x, y_{i, <t})}, 1 - \varepsilon, 1 + \varepsilon) \hat{A_{i, t}]} - \beta \mathbb{D}_{KL}(\pi_{\theta} || \pi_{\theta_{old}})\}}}$$
$\beta, \varepsilon$ - гиперпараметры, $\hat{A_{i, t}}$ - выигрыш относительно средней ожидаемой награды.

\section{Предлагаемое решение}
\subsection{Совмещение SFT и RL}
В данной работе рассматривается подбор оптимальной стратегии чередования этапов SFT и RL.
\paragraph{SFT} На данном этапе модель обучается на стандартной задаче обучения с учителем, минимизируя кросс-энтропийную функцию потерь:
$$\mathcal{L}_{SFT}(\theta) = -\mathbb{E}_{(x, y) \sim \mathcal{D}} \sum_{t = 1}^T \log \pi_{\theta}(y_t | x, y_{<t})$$
Это способствует запоминанию общих подходов к решению и базовых знаний.

\paragraph{RL} В ходе стадии RL модель обучается на следующую функцию потерь: \\
$\mathcal{L}_{GRPO}(\theta) = -\mathbb{E}_{x \sim \mathcal{D}, \{y_i\}_{i = 1}^G \sim \pi_{\theta}(\cdot | x)}$
$${\frac{1}{G} \sum_{i = 1}^G {\frac{1}{T_{y_i}} \sum_{t = 1}^{T_{y_i}} \{\min [\frac{\pi_{\theta}(y_{i, t} | x, y_{i, <t})}{\pi_{\theta_{old}}(y_{i, t} | x, y_{i, <t})} \hat{A_{i, t}}, \text{clip} (\frac{\pi_{\theta}(y_{i, t} | x, y_{i, <t})}{\pi_{\theta_{old}}(y_{i, t} | x, y_{i, <t})}, 1 - \varepsilon, 1 + \varepsilon) \hat{A_{i, t}]} - \beta \mathbb{D}_{KL}(\pi_{\theta} || \pi_{\theta_{old}})\}}}$$
Этот этап используется для улучшения обобщающей способности модели, помогая ей применять свои знания для решения новых задач.

\paragraph{SFT + RL} В качестве стратегии совмещения двух стадий рассматриваются следующие варианты:
\begin{enumerate}
    \item Использование комбинированной функции потерь:
    $$\mathcal{L}_{total}(\theta) = \lambda\mathcal{L}_{SFT}(\theta) + (1 - \lambda)\mathcal{L}_{RL}(\theta) , \lambda \in [0, 1]$$
    \item Переключение между стадиями на основе анализа метрик валидации - темпа изменения функции потерь, стабильности KL-дивергенции, уровня разнообразия ответов. В данном случае выбирается, какая продолжительность той или иной стадии является оптимальной. Является частным случаем первой стратегии с $\lambda \in \{0, 1\}$
\end{enumerate}

\subsection{Энтропийная регуляризация}
Стадия RL является достаточно стохастичной, что может привести к "коллапсу энтропии", когда энтропия распределения становится близка к нулю, и политика становится близка к детерминированной. Чтобы этого избежать, рассматривается влияние регуляризационного члена, имеющего вид:
$$R = \beta H(\pi_{\theta}(y | x)), \quad H(\pi_{\theta}(y | x)) = -\sum_{t = 1}^T {\pi_{\theta} (y_t | x, y_{<t}) \log{\pi_{\theta} (y_t | x, y_{<t})}}$$
Здесь $\beta$ - гиперпараметр регуляризации, $H$ - энтропия распределения.

\section{Эксперименты}
Основной целью экспериментов из данной секции является получение качества, сравнимого с имеющимися результатами на известных бенчмарках, оценивающих математическое рассуждение языковых моделей.

\subsection{Бенчмарки}
Так как в данной работе используются небольшие модели (0.5B), для оценки качества были выбраны популярные бенчмарки, содержащие математические задачи уровня средней и старшей школы - \texttt{GSM8K, MATH-500} (\citep{GSM8K, MATH})

\paragraph{GSM8K} - это датасет, содержащий около 8 500 задач по арифметике и элементарной математике, сформулированных в виде текстовых описаний на английском языке. Каждое задание сопровождается подробным решением в пошаговом виде и корректным числовым ответом.

\paragraph{MATH-500} - это сокращённая версия набора \texttt{MATH}, включающая 500 задач повышенной сложности, охватывающих широкий спектр разделов школьной и олимпиадной математики: алгебру, геометрию, комбинаторику, теорию чисел и вероятности. Каждая задача содержит подробное текстовое описание и ожидаемый итоговый ответ в числовой или символьной форме.

\subsection{Бейзлайны}
Для объективной оценки эффективности предложенного подхода проводилось сравнение с рядом современных методов дообучения больших языковых моделей с использованием обучения с подкреплением. В качестве бейзлайнов были выбраны три наиболее релевантных решения: ReLIFT, SuperRL и DeepSeekMath.

\paragraph{ReLIFT} - гибридный подход, в котором основное обучение происходит при помощи RL, а для наиболее сложных задач используется SFT

\paragraph{SuperRL} - метод, схожий по своей структуре с предыдущим. 

\paragraph{DeepSeekMath} - классический подход, сочетающий SFT и GRPO

Результаты данных методов на рассматриваемых бенчмарках представлены в таблице ():

\begin{table}[t]
\centering
\label{tab:results}
\begin{tabular}{lcc}
\toprule
\textbf{Модель} & \textbf{GSM8K} & \textbf{MATH-500} \\
\midrule
ReLIFT & - & 87.4 \\
SuperRL & 78.9 & - \\
DeepSeekMath & 64.2 & - \\
\bottomrule
\end{tabular}
\caption{Результаты бейзлайнов на GSM8K и MATH-500 (Exact Match, \%).}
\end{table}






\section{Examples of citations, figures, tables, references}
\label{sec:others}

\subsection{Citations}
Citations use \verb+natbib+. The documentation may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{../figures/log_reg_cs_exp.eps}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}


\begin{table}
	\caption{Sample table title}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size ($\mu$m) \\
		\midrule
		Dendrite & Input terminal  & $\sim$100     \\
		Axon     & Output terminal & $\sim$10      \\
		Soma     & Cell body       & up to $10^6$  \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
	\item Lorem ipsum dolor sit amet
	\item consectetur adipiscing elit.
	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}

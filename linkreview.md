
| Название                                          | Ссылка                                                                                                                                                                                                         | Краткий вывод                                                                           | Обозначение |
| ------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | ----------- |
| DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models                                          | [link](https://arxiv.org/pdf/2402.03300)                                                                                                                                    | Основополагающая статья  про метод GRPO и обучение сравнительно небольшой модели на решение математических задач                | ✅           |
| A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning                                       | [link](https://arxiv.org/pdf/2507.08267)                                                                                                                           | Статья о совмещении SFT и GRPO при обучении LLM путем их корректного совмещения                                            | ✅✅          |
| LEARNING WHAT REINFORCEMENT LEARNING CAN’T: INTERLEAVED ONLINE FINE-TUNING FOR HARDEST QUESTIONS                                     | [link](https://arxiv.org/pdf/2506.07527)                                                                                                                                         | Использовали RL для общего обучения модели, после чего добились повышения качества с помощью SFT для наиболее тяжелых вопросов | ✅✅          |
| SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning                          | [link](https://arxiv.org/pdf/2506.01096)                                                                                                                                                                                     | Предложили фреймворк, который позволяет адаптивно переключаться между RL и SFT | ✅         |
| Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs                                           | [link](https://arxiv.org/pdf/2505.13026)                                                                                                                                     | Еще один фреймворк, который динамически балансирует между SFT и RL в процессе обучения. Обучение начинается с SFT, после чего в него встраивается GRPO             | ✅           |
| Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning                                          | [link](https://arxiv.org/pdf/2509.06948)                                                                                                                                       | Совмещение стадий SFT и RL, позволяющее им взаимодействовать друг с другом, влияя на результат обучения          | ✅✅          |
| Distilling the Implicit Multi-Branch Structure in LLMs’ Reasoning via Reinforcement Learning                                           | [link](https://arxiv.org/pdf/2505.16142)                                                                                                                                        | Фреймворк, позволяющий производить дистилляцию с помощью RL, что позволяет сохранять сложную структуру рассуждения модели-учителя  | ✅           |
| Deep Reinforcement Learning from Human Preferences                                         | [link](https://arxiv.org/pdf/1706.03741)                                                                                                                                     | Основополагающая статья про метод RLHF  | ✅           |
| Proximal Policy Optimization Algorithms                                             | [link](https://arxiv.org/pdf/1707.06347)                                                                                                                                                                                     | Основополагающая статья про метод PPO              | ✅           |
| UNVEILING THE SECRET RECIPE: A GUIDE FOR SUPERVISED FINE-TUNING SMALL LLMS                                            | [link](https://arxiv.org/pdf/2412.13337)                                                                                                                                    | Практические особенности SFT для LLM с малым числом параметров              | ✅           |
| MITIGATING FORGETTING BETWEEN SUPERVISED AND REINFORCEMENT LEARNING YIELDS STRONGER REASONERS                                            | [link](https://arxiv.org/pdf/2510.04454)                                                                                                                                                                                     | Фреймворк для динамической интеграции SFT и RL. Решает проблему забывания знаний, полученных из RL, в ходе SFT                                | ✅           |
| Chain-of-Thought Prompting Elicits Reasoning in Large Language Models                                            | [link](https://arxiv.org/pdf/2201.11903)                                                                                                                                                                                  | Введение в Chain-of-thought подход          | ✅          |
| Training Verifiers to Solve Math Word Problems                                          | [link](https://arxiv.org/pdf/2110.14168)                                                                                                    | Датасет с математическими задачами для оценивания математического мышления языковых моделей                 | ✅          |
| Measuring Mathematical Problem Solving With the MATH Dataset                                       | [link](https://arxiv.org/pdf/2103.03874)                                                                                                                                    | Математический датасет с пошаговыми решениями + датасет для предобучения фундаментальным задачам математики.                 | ✅          |
| Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge                                 | [link](https://arxiv.org/pdf/1803.05457)                                                                                                                                                                                    | Корпус, содержащий набор научных задач, проверяющих логические рассуждения моделей         | ✅          |
| On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification                                            | [link](https://arxiv.org/html/2508.05629v1)                                      | Подход, позволяющий улучшить обычный SFT, борясь с ограничением на обобщающую способность модели без использования RL                | ✅          |
| RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs                                          | [link](https://arxiv.org/html/2508.16546v1)                                                                                                   | Исследование, направленное на изучение влияния SFT и RL на дообучение больших языковых моделей                  | ✅          |
| Training-Free Group Relative Policy Optimization                              | [link](https://arxiv.org/pdf/2510.08191)                                                                                                                                                                                     | Улучшение GRPO с точки зрения ресурсоэффективности                                   | ✅          |
| SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training                                            | [link](https://openreview.net/pdf?id=dYur3yabMj)                                                                                                                                  | Исследование на тему роли каждого из этапов дообучения (SFT и RL) на примере различных сред                      | ✅          |
| DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning                                   | [link](https://arxiv.org/pdf/2501.12948)                                                                                                                                                   | Основополагающая статья про обучение модели без этапа SFT, а только с использованием RL                   | ✅          |
| OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems                                      | [link](https://arxiv.org/pdf/2402.14008)                                                                                                                                                                                     | Бенчмарк с олимпиадными задачами по физике и математике            | ✅          |
| DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning                                             | [link](https://arxiv.org/pdf/2504.11456)                                                                                                                                                                                     | Математический датасет с задачами повышенной сложности                    | ✅          |
| Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment                             | [link](https://arxiv.org/pdf/2405.17888)                                                                     | Метод Inverse Reinforcement Learning, приводящий к новым алгоритмам SFT, являющимся эффективными в реализации и устойчивыми к некачественным данным                    | ✅          |
| Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models                                   | [link](https://arxiv.org/pdf/2401.01335)                                                                                                        | Новый метод SFT, показывающий результаты, сравнимые с дообучением с помощью RL                          | ✅          |
| LIMR: Less is More for RL Scaling                                  | [link](https://arxiv.org/pdf/2502.11886)                                                                                                                                                                                 | Автоматический подход к выбору данных для RL + демонстрация того, что тщательно отобранная выборка данных может соперничать с полной коллекцией и даже превосходить ее на некоторых бенчмарках                           | ✅          |
| Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn’t                                  | [link](https://arxiv.org/pdf/2503.16219) | Исследование на тему использования RL при обучении LLM с малым числом параметров          | ✅          |
| Training Compute-Optimal Large Language Models                       | [link](https://arxiv.org/pdf/2203.15556)                                                                                                                  | Исследование на тему оптимального размера модели, количества токенов в соответствии с имеющимися вычислительными ресурсами                                   | ✅          |
| Direct Preference Optimization: Your Language Model is Secretly a Reward Model                                    | [link](https://arxiv.org/pdf/2305.18290)                                                                                                                                                  | Основополагающая статья про метод DPO          | ✅          |
| SCALING REINFORCEMENT LEARNING WITH LLMS                                              | [link](https://arxiv.org/pdf/2501.12599)                                                                                                                                      | Модель, обученная с помощью нового подхода к RL                      | ✅           |
|Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond                           | [link](https://arxiv.org/pdf/2503.10460)                                                                        | Предложили систему обучения моделей с длинной цепочкой рассуждения с использованием SFT и DPO                 | ✅           |
|SCALING RELATIONSHIP ON LEARNING MATHEMATICAL REASONING WITH LARGE LANGUAGE MODELS                           | [link](https://arxiv.org/pdf/2308.01825)                                                                        | SFT для улучшения математических рассуждений больших языковых моделей                 | ✅           |
